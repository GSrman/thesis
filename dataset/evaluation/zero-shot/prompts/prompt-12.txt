For the following research paper segments, make a short bulletpoint summary of the following ethical aspects: Bias and fairness, Privacy, Transparency, Dual Use, Accessibility, Data collection. If an aspect is not discussed, say so.

Abstract
Because of their superior ability to preserve sequence information over time,
Long Short-Term Memory (LSTM) networks, a type of recurrent neural network with a more complex computational
unit, have obtained strong results on a variety of sequence modeling tasks. The
only underlying LSTM structure that has
been explored so far is a linear chain.
However, natural language exhibits syntactic properties that would naturally combine words to phrases. We introduce the
Tree-LSTM, a generalization of LSTMs to
tree-structured network topologies. TreeLSTMs outperform all existing systems
and strong LSTM baselines on two tasks:
predicting the semantic relatedness of two
sentences (SemEval 2014, Task 1) and
sentiment classification (Stanford Sentiment Treebank).
 Introduction
Most models for distributed representations of
phrases and sentences—that is, models where realvalued vectors are used to represent meaning—fall
into one of three classes: bag-of-words models,
sequence models, and tree-structured models. In
bag-of-words models, phrase and sentence representations are independent of word order; for example, they can be generated by averaging constituent word representations (Landauer and Dumais, 1997; Foltz et al., 1998). In contrast, sequence models construct sentence representations
as an order-sensitive function of the sequence of
tokens (Elman, 1990; Mikolov, 2012). Lastly,
tree-structured models compose each phrase and
sentence representation from its constituent subphrases according to a given syntactic structure
over the sentence (Goller and Kuchler, 1996;
Socher et al., 2011).
Order-insensitive models are insufficient to
fully capture the semantics of natural language
due to their inability to account for differences in
meaning as a result of differences in word order
or syntactic structure (e.g., “cats climb trees” vs.
“trees climb cats”). We therefore turn to ordersensitive sequential or tree-structured models. In
particular, tree-structured models are a linguistically attractive option due to their relation to syntactic interpretations of sentence structure. A natural question, then, is the following: to what extent (if at all) can we do better with tree-structured
models as opposed to sequential models for sentence representation? In this paper, we work towards addressing this question by directly comparing a type of sequential model that has recently
been used to achieve state-of-the-art results in several NLP tasks against its tree-structured generalization.
Due to their capability for processing arbitrarylength sequences, recurrent neural networks
(RNNs) are a natural choice for sequence modeling tasks. Recently, RNNs with Long Short-Term
Memory (LSTM) units (Hochreiter and Schmidhuber, 1997) have re-emerged as a popular architecture due to their representational power and effectiveness at capturing long-term dependencies.
LSTM networks, which we review in Sec. 2, have
been successfully applied to a variety of sequence
modeling and prediction tasks, notably machine
translation (Bahdanau et al., 2015; Sutskever et al.,
2014), speech recognition (Graves et al., 2013),
image caption generation (Vinyals et al., 2014),
and program execution (Zaremba and Sutskever,
2014).
In this paper, we introduce a generalization of
the standard LSTM architecture to tree-structured
network topologies and show its superiority for
representing sentence meaning over a sequential
LSTM. While the standard LSTM composes its
hidden state from the input at the current time
step and the hidden state of the LSTM unit in the
previous time step, the tree-structured LSTM, or
Tree-LSTM, composes its state from an input vector and the hidden states of arbitrarily many child
units. The standard LSTM can then be considered
a special case of the Tree-LSTM where each internal node has exactly one child.
In our evaluations, we demonstrate the empirical strength of Tree-LSTMs as models for representing sentences. We evaluate the Tree-LSTM
architecture on two tasks: semantic relatedness
prediction on sentence pairs and sentiment classification of sentences drawn from movie reviews.
Our experiments show that Tree-LSTMs outperform existing systems and sequential LSTM baselines on both tasks. Implementations of our models and experiments are available at https://
github.com/stanfordnlp/treelstm.
 Conclusion
In this paper, we introduced a generalization of
LSTMs to tree-structured network topologies. The
Tree-LSTM architecture can be applied to trees
with arbitrary branching factor. We demonstrated
the effectiveness of the Tree-LSTM by applying
the architecture in two tasks: semantic relatedness
and sentiment classification, outperforming existing systems on both. Controlling for model dimensionality, we demonstrated that Tree-LSTM
models are able to outperform their sequential
counterparts. Our results suggest further lines of
work in characterizing the role of structure in producing distributed representations of sentences.

