For the following research paper segments, make a short bulletpoint summary of the following ethical aspects: Bias and fairness, Privacy, Transparency, Dual Use, Accessibility, Data collection. If an aspect is not discussed, say so.

Abstract
This paper shows that pretraining multilingual
language models at scale leads to significant
performance gains for a wide range of crosslingual transfer tasks. We train a Transformerbased masked language model on one hundred
languages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed
XLM-R, significantly outperforms multilingual
BERT (mBERT) on a variety of cross-lingual
benchmarks, including +14.6% average accuracy on XNLI, +13% average F1 score on
MLQA, and +2.4% F1 score on NER. XLM-R
performs particularly well on low-resource languages, improving 15.7% in XNLI accuracy
for Swahili and 11.4% for Urdu over previous XLM models. We also present a detailed
empirical analysis of the key factors that are
required to achieve these gains, including the
trade-offs between (1) positive transfer and capacity dilution and (2) the performance of high
and low resource languages at scale. Finally,
we show, for the first time, the possibility of
multilingual modeling without sacrificing perlanguage performance; XLM-R is very competitive with strong monolingual models on the
GLUE and XNLI benchmarks. We will make
our code, data and models publicly available.1
 Introduction
The goal of this paper is to improve cross-lingual
language understanding (XLU), by carefully studying the effects of training unsupervised crosslingual representations at a very large scale. We
present XLM-R a transformer-based multilingual
masked language model pre-trained on text in 100
languages, which obtains state-of-the-art performance on cross-lingual classification, sequence labeling and question answering.
âˆ—Equal contribution.
Correspondence to {aconneau,kartikayk}@fb.com
1
https://github.com/facebookresearch/(fairseq-py,pytext,xlm)
Multilingual masked language models (MLM)
like mBERT (Devlin et al., 2018) and XLM (Lample and Conneau, 2019) have pushed the stateof-the-art on cross-lingual understanding tasks
by jointly pretraining large Transformer models (Vaswani et al., 2017) on many languages.
These models allow for effective cross-lingual
transfer, as seen in a number of benchmarks including cross-lingual natural language inference
(Bowman et al., 2015; Williams et al., 2017; Conneau et al., 2018), question answering (Rajpurkar
et al., 2016; Lewis et al., 2019), and named entity recognition (Pires et al., 2019; Wu and Dredze,
2019). However, all of these studies pre-train on
Wikipedia, which provides a relatively limited scale
especially for lower resource languages.
In this paper, we first present a comprehensive
analysis of the trade-offs and limitations of multilingual language models at scale, inspired by recent monolingual scaling efforts (Liu et al., 2019).
We measure the trade-off between high-resource
and low-resource languages and the impact of language sampling and vocabulary size. The experiments expose a trade-off as we scale the number
of languages for a fixed model capacity: more languages leads to better cross-lingual performance
on low-resource languages up until a point, after
which the overall performance on monolingual and
cross-lingual benchmarks degrades. We refer to
this tradeoff as the curse of multilinguality, and
show that it can be alleviated by simply increasing model capacity. We argue, however, that this
remains an important limitation for future XLU
systems which may aim to improve performance
with more modest computational budgets.
Our best model XLM-RoBERTa (XLM-R) outperforms mBERT on cross-lingual classification by
up to 23% accuracy on low-resource languages. It
outperforms the previous state of the art by 5.1% average accuracy on XNLI, 2.42% average F1-score
on Named Entity Recognition, and 9.1% average
F1-score on cross-lingual Question Answering. We
also evaluate monolingual fine tuning on the GLUE
and XNLI benchmarks, where XLM-R obtains results competitive with state-of-the-art monolingual
models, including RoBERTa (Liu et al., 2019).
These results demonstrate, for the first time, that
it is possible to have a single large model for all
languages, without sacrificing per-language performance. We will make our code, models and data
publicly available, with the hope that this will help
research in multilingual NLP and low-resource language understanding.
Conclusion
In this work, we introduced XLM-R, our new state
of the art multilingual masked language model
trained on 2.5 TB of newly created clean CommonCrawl data in 100 languages. We show that it
provides strong gains over previous multilingual
models like mBERT and XLM on classification,
sequence labeling and question answering. We exposed the limitations of multilingual MLMs, in
particular by uncovering the high-resource versus
low-resource trade-off, the curse of multilinguality
and the importance of key hyperparameters. We
also expose the surprising effectiveness of multilingual models over monolingual models, and show
strong improvements on low-resource languages.
